{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ajain7\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "- Imports Done\n"
     ]
    }
   ],
   "source": [
    "from preprocess.arpit_v2 import *\n",
    "from preprocess.preprocess_v2 import *\n",
    "from preprocess.preprocess_v2 import preprocess\n",
    "import os\n",
    "import inspect\n",
    "import time\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "from models.LDA_multi_level import lda_model_multi_level\n",
    "from models.LDA_single_level import lda_model_single_level\n",
    "print('------------------------------------------------------')\n",
    "print('- Imports Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Imported Data\n"
     ]
    }
   ],
   "source": [
    "# DATA\n",
    "# Note that raw docs is a numpy array. \n",
    "# Example element is: \n",
    "# 'Logical Disk Free Space is low, Description: The disk C: on computer sjcphxstg02.strykercorp.com is running out of disk space. The values that exceeded the thre'\n",
    "# data_file_string = 'short_description.pkl'\n",
    "data_file_string = 'data.pkl'\n",
    "data_file = os.path.join(os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe()))),'data',data_file_string)\n",
    "raw_docs = pickle.load(open(data_file,'rb'))\n",
    "print('- Imported Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING\n",
    "preprocess_steps_and_order = {\n",
    "\t'make_lowercase': [True],\n",
    "\t'punctuation_removal':[True],\n",
    "\t'whitespace_removal': [True],\n",
    "\t'store_alphanumeric': [False],\n",
    "\t'pos_removal_nltk': [True],\n",
    "\t'tokenization_nltk': [False],\n",
    "\t'lemmatization_tokenization_spacy': [True],\n",
    "\t'stopwords_removal_nltk': [True],\n",
    "\t'stopwords_removal_spacy': [False],\n",
    "\t'make_bigrams_gensim':[True, {'make_bigrams_gensim': True, 'bigrams_min_count': 10, 'bigrams_threshold': 10}],\n",
    "\t'make_trigrams_gensim':[True, {'make_trigrams_gensim': True, 'trigrams_min_count': 10, 'trigrams_threshold': 10}],\n",
    "\t'min_max_length_removal':[False, {'min_max_length_removal': False, 'mmlr_min_len': 3, 'mmlr_max_len': 50, 'mmlr_deacc': False}]\n",
    "\t}\n",
    "\n",
    "preprocess_functions = {\n",
    "\t'make_lowercase': make_lowercase,\n",
    "\t'punctuation_removal': punctuation_removal,\n",
    "\t'whitespace_removal': whitespace_removal,\n",
    "\t'store_alphanumeric': store_alphanumeric,\n",
    "\t'pos_removal_nltk': pos_removal_nltk,\n",
    "\t'tokenization_nltk': tokenization_nltk,\n",
    "\t'lemmatization_tokenization_spacy': lemmatization_tokenization_spacy,\n",
    "\t'stopwords_removal_nltk': stopwords_removal_nltk,\n",
    "\t'stopwords_removal_spacy': stopwords_removal_spacy,\n",
    "\t'make_bigrams_gensim': make_bigrams_gensim,\n",
    "\t'make_trigrams_gensim': make_trigrams_gensim,\n",
    "\t'min_max_length_removal': min_max_length_removal\n",
    "\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS\n",
    "models_dict = {\n",
    "\t'LDA_single_level': lda_model_single_level,\n",
    "\t'LDA_multi_level': lda_model_multi_level,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIFICATIONS\n",
    "specifications = {\n",
    "\t# 'model':'LDA_single_level', # Can be LDA_multi_level\n",
    "\t'level':2,\n",
    "\t'num_topics_list_level_1':[5,10,15,20,25],\n",
    "\t'num_topics_list_level_2':[3,5,8,11],\n",
    "\t'num_topics_list_level_3':[1,2,3,4,5],\n",
    "\t'coherence':'c_v',\n",
    "\t'need_best_topic': True,\n",
    "\t'model_selection_metric':'coherence', # or 'perplexity',\n",
    "\t'debug':False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************\n",
      "- Starting preprocessing\n",
      "\n",
      "       ##### Lowercasing Done! Time Taken -  0.028003931045532227\n",
      "\n",
      "       ##### Punctuation removed! Time Taken -  0.2020106315612793\n",
      "\n",
      "       ##### Whitespace removed! Time Taken -  0.08400464057922363\n",
      "\n",
      "       ##### POS Removal Done! Time Taken -  40.13323187828064\n",
      "\n",
      "       ##### Lemmatization and Tokenization Done using Spacy! Time Taken -  76.04995536804199\n",
      "\n",
      "       ##### Stopwords Removed using NLTK! Time Taken -  0.3080465793609619\n",
      "\n",
      "       ##### Bi-Grams made using Gensim! Time Taken -  2.110252857208252\n",
      "\n",
      "       ##### Tri-Grams made using Gensim! Time Taken -  1.9426908493041992\n",
      "~~~ pre-processing done in  120.91414070129395\n",
      " \n",
      "- Creating dictionary and corpus\n"
     ]
    }
   ],
   "source": [
    "print('*****************************************************')\n",
    "print('- Starting preprocessing')\n",
    "dictionary, corpus, doc_list = preprocess(\n",
    "\t\t\t\t\t\t\t\traw_docs = raw_docs, \n",
    "\t\t\t\t\t\t\t\tpreprocess_functions = preprocess_functions, \n",
    "\t\t\t\t\t\t\t\tpreprocess_steps_and_order = preprocess_steps_and_order, \n",
    "\t\t\t\t\t\t\t\tdebug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************\n",
      "- Starting model training\n",
      " \n",
      "Sample data point:  ['dataset', 'transaction', 'credit', 'card', 'september', 'cardholder', 'dataset', 'transaction', 'day', 'fraud', 'transaction', 'dataset', 'class', 'fraud', 'account', 'transaction', 'input', 'variable', 'result', 'transformation', 'confidentiality', 'issue', 'feature', 'background', 'datum', 'feature', 'v28', 'component', 'pca', 'feature', 'pca', 'transaction', 'transaction', 'dataset', 'feature', 'amount', 'transaction', 'amount', 'feature', 'dependant', 'cost', 'feature', 'class', 'response', 'variable', 'value', 'case', 'fraud', 'class', 'imbalance', 'ratio', 'accuracy', 'area', 'precision', 'recall', 'curve', 'confusion', 'matrix', 'accuracy', 'classification', 'dataset', 'research', 'collaboration', 'worldline', 'machine', 'group', 'ac', 'libre', 'bruxelle', 'datum_mining', 'fraud', 'detection', 'detail', 'project', 'topic', 'http', 'mlg', 'brufence', 'http', 'mlg', 'dal', 'pozzolo', 'johnson', 'gianluca', 'bontempi', 'probability', 'classification', 'symposium', 'intelligence', 'datum_mining', 'cidm', 'ieee']\n",
      " \n",
      "\t### Running LDA for number of topic - 5\n",
      "\tLDA Done for 5 topic! Time Taken is 13.046070337295532\n",
      "\tEvaluating model for number of topic - 5\n",
      "---\n",
      "\t### Running LDA for number of topic - 10\n",
      "\tLDA Done for 10 topic! Time Taken is 17.90545344352722\n",
      "\tEvaluating model for number of topic - 10\n",
      "---\n",
      "\t### Running LDA for number of topic - 15\n",
      "\tLDA Done for 15 topic! Time Taken is 25.39391589164734\n",
      "\tEvaluating model for number of topic - 15\n",
      "---\n",
      "- Done training model on all topics in 84.21873641014099 sec!\n",
      "Done Single-Level LDA\n"
     ]
    }
   ],
   "source": [
    "print('*****************************************************')\n",
    "print('- Starting model training')\n",
    "lda_dict = lda_model_single_level(\n",
    "\t\t\t\t\tdictionary = dictionary,\n",
    "\t\t\t\t\tcorpus = corpus,\n",
    "\t\t\t\t\tdoc_list = doc_list,\n",
    "\t\t\t\t\tnum_topics_list_level_1 = specifications['num_topics_list_level_1'], \n",
    "\t\t\t\t\tcoherence = specifications['coherence'],\n",
    "\t\t\t\t\tdebug = specifications['debug'],\n",
    "\t\t\t\t\tneed_best_topic = specifications['need_best_topic'],\n",
    "\t\t\t\t\tmodel_selection_metric = specifications['model_selection_metric']\n",
    "\t\t\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0.47154971390590406\n",
      "-8.58700882191759\n"
     ]
    }
   ],
   "source": [
    "# pyLDAvis.enable_notebook()\n",
    "# print(lda_dict['best_topic'])\n",
    "# print(lda_dict['coherence_score'])\n",
    "# print(lda_dict['perplexity_score'])\n",
    "# visualization = pyLDAvis.gensim.prepare(lda_dict['best_lda_model'], lda_dict['corpus'], lda_dict['dictionary'])\n",
    "# pyLDAvis.save_html(visualization, 'lda.html')\n",
    "# visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*****************************************************')\n",
    "print('- Starting model training')\n",
    "lda_level_1, lda_level_2 = lda_model_multi_level(\n",
    "\t\t\t\t\tlevel = specifications['level'],\n",
    "\t\t\t\t\tdictionary = dictionary,\n",
    "\t\t\t\t\tcorpus = corpus,\n",
    "\t\t\t\t\tdoc_list = doc_list,\n",
    "\t\t\t\t\tcoherence = specifications['coherence'],\n",
    "\t\t\t\t\tdebug = specifications['debug'],\n",
    "\t\t\t\t\tneed_best_topic = specifications['need_best_topic'],\n",
    "\t\t\t\t\tmodel_selection_metric = specifications['model_selection_metric'],\n",
    "\t\t\t\t\tnum_topics_list_level_1 = specifications['num_topics_list_level_1'], \n",
    "\t\t\t\t\tnum_topics_list_level_2 = specifications['num_topics_list_level_2'], \n",
    "\t\t\t\t\t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
